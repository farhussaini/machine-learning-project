# -*- coding: utf-8 -*-
"""worked.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10yJBu69kxpftZHyftBSHC4ee2S0LoynA
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd
import numpy as np
import glob
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Conv1D, MaxPooling1D, Dropout, Flatten
from tensorflow.keras.optimizers import Adam

!pip install xgboost tensorflow statsmodels
!pip install tensorflow

folder_path ='/content/drive/MyDrive/MLProject2025/MLPro'   # Update the folder path as per your setup

# Get all CSV files in the folder
csv_files = glob.glob(os.path.join(folder_path, '*.csv'))

# Process each file
for file in csv_files:
    print(f"Processing file: {file}")

    df = pd.read_csv(file)

# Define file paths
base_path = "/content/drive/MyDrive/MLProject2025/MLPro/"

files = {
    "campus_meta": "campus_meta.csv",
    "calendar": "calender.csv",
    "gas": "gas_consumption.csv",
    "events": "events.csv",
    "building_meta": "building_meta.csv",
    "building_consumption": "building_consumption.csv",
    "building_submeter": "building_submeter_consumption.csv",
    "nmi_consumption": "nmi_consumption.csv",
    "water": "water_consumption.csv",
    "nmi_meta": "nmi_meta.csv",
    "weather": "weather_data.csv"
}

# Load all files
dfs = {name: pd.read_csv(base_path + fname) for name, fname in files.items()}

# Print column names to debug
for name, df in dfs.items():
    print(f"{name} columns: {df.columns.tolist()}")

# Step 1: Convert 'timestamp' columns to datetime with UTC
timestamped_keys = ["gas", "water", "nmi_consumption", "building_submeter", "weather"]
for key in timestamped_keys:
    dfs[key]['timestamp'] = pd.to_datetime(dfs[key]['timestamp'], utc=True)

# Step 2: Resampling & Filling function with deduplication
def resample_and_fill(df, time_col, freq='h'):
    df = df.dropna(subset=[time_col])
    df = df.sort_values(by=time_col)
    df = df.drop_duplicates(subset=[time_col])  # <- KEY FIX
    df = df.set_index(time_col)
    df_resampled = df.resample(freq).ffill().bfill()
    return df_resampled

# Apply to datasets with timestamp
df_gas = resample_and_fill(dfs["gas"], 'timestamp')
df_water = resample_and_fill(dfs["water"], 'timestamp')
df_nmi = resample_and_fill(dfs["nmi_consumption"], 'timestamp')
df_weather = resample_and_fill(dfs["weather"], 'timestamp')
df_building = resample_and_fill(dfs["building_submeter"], 'timestamp')

# Step 3: Remove Outliers using IQR (Interquartile Range)
def remove_outliers_iqr(df, cols):
    for col in cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        df = df[(df[col] >= lower) & (df[col] <= upper)]
    return df

df_gas = remove_outliers_iqr(df_gas, ['consumption'])
df_water = remove_outliers_iqr(df_water, ['consumption'])
df_nmi = remove_outliers_iqr(df_nmi, ['consumption', 'demand_kW', 'demand_kVA'])
df_building = remove_outliers_iqr(df_building, ['consumption', 'current', 'voltage', 'power', 'power_factor'])

# Step 4: Time Feature Engineering
def add_time_features(df):
    df['hour'] = df.index.hour
    df['dayofweek'] = df.index.dayofweek
    df['month'] = df.index.month
    df['is_weekend'] = df['dayofweek'] >= 5
    return df

df_gas = add_time_features(df_gas)
df_water = add_time_features(df_water)
df_nmi = add_time_features(df_nmi)
df_weather = add_time_features(df_weather)
df_building = add_time_features(df_building)

print("✅ All timestamped datasets cleaned, resampled, and feature-engineered.")

df_gas.to_csv("cleaned_gas.csv")
df_water.to_csv("cleaned_water.csv")
df_nmi.to_csv("cleaned_nmi.csv")
df_weather.to_csv("cleaned_weather.csv")
df_building.to_csv("cleaned_building.csv")

# Print columns of df_gas, df_water, and df_building
print("Columns in df_gas:")
print(df_gas.columns)

print("\nColumns in df_water:")
print(df_water.columns)

print("\nColumns in df_building:")
print(df_building.columns)

"""Purpose
Split a DataFrame df into:

Training inputs (X_train) and targets (y_train)

Testing inputs (X_test) and targets (y_test)
…based on a forecast horizon (how far ahead we want to predict).
"""

def train_test_split_horizon(df, horizon, target_col='consumption'):
    df = df.sort_index()
    # Use enough data for training, at least horizon + some buffer
    min_data_points = horizon + 2 * horizon  # horizon + 2 times horizon for training
    df = df[-min_data_points:]
    train_size = len(df) - horizon
    train = df.iloc[:train_size]
    test = df.iloc[train_size:]
    X_train, y_train = train.drop(columns=[target_col]), train[target_col]
    X_test, y_test = test.drop(columns=[target_col]), test[target_col]
    return X_train, X_test, y_train, y_test

def evaluate_model(model, X_test, y_test):
    if hasattr(model, 'forecast'):
        print("Using forecast method")
        predictions = model.forecast(steps=len(y_test))
    else:
        print("Using predict method")
        predictions = model.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, predictions))
    return rmse

# TCN model builder
def build_tcn_model(input_shape):
    model = Sequential()
    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='causal', input_shape=input_shape))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Dropout(0.2))
    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='causal'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(1))
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
    return model

# Define horizons (in hours)
horizons = {
    "short_1d": 24,
    "medium_7d": 168,
    "long_30d": 720
}

# To store RMSE values for plotting
rmse_results = {
    'Model': [],
    'RMSE': [],
    'Dataset': [],
    'Horizon': []
}

# Iterate through the datasets and horizons, evaluate the models and collect RMSE
for df_name, df in zip(['df_gas', 'df_water', 'df_building'], [df_gas, df_water, df_building]):
    for horizon_name, horizon in horizons.items():
        print(f"--- {df_name.upper()} | {horizon_name.upper()} FORECAST ---")

        # Prepare train/test
        X_train, X_test, y_train, y_test = train_test_split_horizon(df, horizon)

        # 1. ARIMA - Reduced model order
        try:
            arima_model = ARIMA(y_train, order=(1, 1, 1))
            arima_fit = arima_model.fit()
            arima_rmse = evaluate_model(arima_fit, X_test, y_test)
            print(f"ARIMA RMSE: {arima_rmse:.4f}")
            rmse_results['Model'].append('ARIMA')
            rmse_results['RMSE'].append(arima_rmse)
            rmse_results['Dataset'].append(df_name)
            rmse_results['Horizon'].append(horizon_name)
        except np.linalg.LinAlgError:
            print("ARIMA model failed to converge.")

        # 2. SARIMA
        sarima_model = SARIMAX(y_train, order=(5, 1, 0), seasonal_order=(1, 1, 1, 24))
        sarima_fit = sarima_model.fit(disp=False)
        sarima_rmse = evaluate_model(sarima_fit, X_test, y_test)
        print(f"SARIMA RMSE: {sarima_rmse:.4f}")
        rmse_results['Model'].append('SARIMA')
        rmse_results['RMSE'].append(sarima_rmse)
        rmse_results['Dataset'].append(df_name)
        rmse_results['Horizon'].append(horizon_name)

        # 3. Exponential Smoothing
        es_model = ExponentialSmoothing(y_train, trend='add', seasonal='add', seasonal_periods=24)
        es_fit = es_model.fit()
        es_rmse = evaluate_model(es_fit, X_test, y_test)
        print(f"Exp. Smoothing RMSE: {es_rmse:.4f}")
        rmse_results['Model'].append('Exp. Smoothing')
        rmse_results['RMSE'].append(es_rmse)
        rmse_results['Dataset'].append(df_name)
        rmse_results['Horizon'].append(horizon_name)

        # 4. Random Forest
        rf_model = RandomForestRegressor(n_estimators=100)
        rf_model.fit(X_train, y_train)
        rf_rmse = evaluate_model(rf_model, X_test, y_test)
        print(f"Random Forest RMSE: {rf_rmse:.4f}")
        rmse_results['Model'].append('Random Forest')
        rmse_results['RMSE'].append(rf_rmse)
        rmse_results['Dataset'].append(df_name)
        rmse_results['Horizon'].append(horizon_name)

        # 5. XGBoost
        xgb_model = xgb.XGBRegressor(n_estimators=100)
        xgb_model.fit(X_train, y_train)
        xgb_rmse = evaluate_model(xgb_model, X_test, y_test)
        print(f"XGBoost RMSE: {xgb_rmse:.4f}")
        rmse_results['Model'].append('XGBoost')
        rmse_results['RMSE'].append(xgb_rmse)
        rmse_results['Dataset'].append(df_name)
        rmse_results['Horizon'].append(horizon_name)

        # Convert to NumPy arrays with correct dtype
        X_train = X_train.astype(np.float32)
        X_test = X_test.astype(np.float32)
        y_train = y_train.astype(np.float32)
        y_test = y_test.astype(np.float32)

        # Reshape for deep learning
        X_train_dl = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))
        X_test_dl = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))

        # 6. LSTM
        lstm_model = Sequential()
        lstm_model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], 1)))
        lstm_model.add(Dense(1))
        lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
        lstm_model.fit(X_train_dl, y_train, epochs=20, batch_size=32, verbose=0)
        lstm_rmse = evaluate_model(lstm_model, X_test_dl, y_test)
        print(f"LSTM RMSE: {lstm_rmse:.4f}")
        rmse_results['Model'].append('LSTM')
        rmse_results['RMSE'].append(lstm_rmse)
        rmse_results['Dataset'].append(df_name)
        rmse_results['Horizon'].append(horizon_name)

        # 7. GRU
        gru_model = Sequential()
        gru_model.add(GRU(50, activation='relu', input_shape=(X_train.shape[1], 1)))
        gru_model.add(Dense(1))
        gru_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
        gru_model.fit(X_train_dl, y_train, epochs=20, batch_size=32, verbose=0)
        gru_rmse = evaluate_model(gru_model, X_test_dl, y_test)
        print(f"GRU RMSE: {gru_rmse:.4f}")
        rmse_results['Model'].append('GRU')
        rmse_results['RMSE'].append(gru_rmse)
        rmse_results['Dataset'].append(df_name)
        rmse_results['Horizon'].append(horizon_name)

        # 8. TCN
        tcn_model = build_tcn_model((X_train.shape[1], 1))
        tcn_model.fit(X_train_dl, y_train, epochs=20, batch_size=32, verbose=0)
        tcn_rmse = evaluate_model(tcn_model, X_test_dl, y_test)
        print(f"TCN RMSE: {tcn_rmse:.4f}")
        rmse_results['Model'].append('TCN')
        rmse_results['RMSE'].append(tcn_rmse)
        rmse_results['Dataset'].append(df_name)
        rmse_results['Horizon'].append(horizon_name)

        print(f"Finished Forecasting for {df_name}\n")

# Convert RMSE results to DataFrame
df_rmse = pd.DataFrame(rmse_results)

"""##################################################"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Convert RMSE results into a DataFrame
rmse_df = pd.DataFrame(rmse_results)

# Set plot style
sns.set(style="whitegrid")

# Create a plot per dataset
for dataset in rmse_df['Dataset'].unique():
    plt.figure(figsize=(12, 6))
    subset = rmse_df[rmse_df['Dataset'] == dataset]

    # Create line plot
    sns.lineplot(
        data=subset,
        x='Horizon',
        y='RMSE',
        hue='Model',
        marker='o',
        palette='tab10'
    )

    # Highlight best RMSE per horizon
    for horizon in subset['Horizon'].unique():
        horizon_data = subset[subset['Horizon'] == horizon]
        min_idx = horizon_data['RMSE'].idxmin()
        best_model = horizon_data.loc[min_idx]
        plt.scatter(
            x=best_model['Horizon'],
            y=best_model['RMSE'],
            s=150,
            c='gold',
            edgecolor='black',
            marker='*',
            label=f"Best: {best_model['Model']} ({horizon})"
        )

    plt.title(f'RMSE Comparison for {dataset}')
    plt.ylabel('RMSE')
    plt.xlabel('Forecast Horizon')
    plt.legend(loc='best', fontsize='small')
    plt.tight_layout()
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error

# Set the Seaborn style for better aesthetics
sns.set(style="whitegrid", palette="muted")

# Define the function to plot actual vs predicted consumption with enhanced visuals
def plot_actual_vs_predicted(df, y_pred, model_name, dataset_name, horizon, identifier='campus_id'):
    """
    Function to plot Actual vs Predicted Consumption with enhanced visualizations.

    Parameters:
    - df: DataFrame containing the actual values (must contain 'consumption' column).
    - y_pred: Array-like, predicted consumption values.
    - model_name: The model used for prediction (e.g., 'RandomForest', 'XGBoost').
    - dataset_name: The name of the dataset (e.g., 'Gas', 'Water', 'Building').
    - horizon: The forecast horizon (e.g., '1d', '7d', '30d').
    - identifier: The identifier column to be used (e.g., 'campus_id', 'meter_id', 'building_id').

    Returns:
    - None
    """
    # Get actual consumption for the corresponding dataset
    y_true = df['consumption'].iloc[-len(y_pred):].values

    plt.figure(figsize=(12, 6))

    # Plot the actual and predicted values
    plt.plot(y_true, label='Actual Consumption', linewidth=2, color='blue', alpha=0.7)
    plt.plot(y_pred, label='Predicted Consumption', linewidth=2, color='orange', alpha=0.7)

    # Add a best-fit line (optional)
    plt.plot(y_true, y_true, '--', color='gray', label='Ideal Prediction', alpha=0.5)

    # Add titles and labels
    plt.title(f'{dataset_name} - {model_name} ({horizon})', fontsize=16)
    plt.xlabel('Time Steps', fontsize=12)
    plt.ylabel('Consumption', fontsize=12)

    # Add a legend
    plt.legend(loc='upper left', fontsize=12)

    # Show RMSE as an annotation on the plot
    rmse = calculate_rmse(y_true, y_pred)
    plt.annotate(f'RMSE: {rmse:.4f}', xy=(0.7, 0.1), xycoords='axes fraction', fontsize=14,
                 bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=1'))

    # Enhance grid and layout
    plt.grid(True)
    plt.tight_layout()

    # Display the plot
    plt.show()

# Function to calculate RMSE
def calculate_rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

# Placeholder for predicted values (replace these with your actual model predictions)
predictions = {
    'Gas': {
        'RandomForest': {
            '1d': np.random.rand(100),
            '7d': np.random.rand(100),
            '30d': np.random.rand(100)
        },
        'XGBoost': {
            '1d': np.random.rand(100),
            '7d': np.random.rand(100),
            '30d': np.random.rand(100)
        },
        'LSTM': {
            '1d': np.random.rand(100),
            '7d': np.random.rand(100),
            '30d': np.random.rand(100)
        }
    },
    'Water': {
        'RandomForest': {
            '1d': np.random.rand(100),
            '7d': np.random.rand(100),
            '30d': np.random.rand(100)
        },
        'XGBoost': {
            '1d': np.random.rand(100),
            '7d': np.random.rand(100),
            '30d': np.random.rand(100)
        },
        'LSTM': {
            '1d': np.random.rand(100),
            '7d': np.random.rand(100),
            '30d': np.random.rand(100)
        }
    },
    'Building': {
        'RandomForest': {
            '1d': np.random.rand(100),
            '7d': np.random.rand(100),
            '30d': np.random.rand(100)
        },
        'XGBoost': {
            '1d': np.random.rand(100),
            '7d': np.random.rand(100),
            '30d': np.random.rand(100)
        },
        'LSTM': {
            '1d': np.random.rand(100),
            '7d': np.random.rand(100),
            '30d': np.random.rand(100)
        }
    }
}

# Replace with your actual dataframes containing actual values for Gas, Water, and Building
df_gas = pd.DataFrame({'consumption': np.random.rand(100)})
df_water = pd.DataFrame({'consumption': np.random.rand(100)})
df_building = pd.DataFrame({'consumption': np.random.rand(100)})

# Calculate RMSE and plot for each model and each dataset
rmse_results = []

for dataset in ['Gas', 'Water', 'Building']:
    for model in ['RandomForest', 'XGBoost', 'LSTM']:
        for horizon in ['1d', '7d', '30d']:
            # Get actual consumption data for the last 100 records
            if dataset == 'Gas':
                y_true = df_gas['consumption'].iloc[-100:].values
            elif dataset == 'Water':
                y_true = df_water['consumption'].iloc[-100:].values
            elif dataset == 'Building':
                y_true = df_building['consumption'].iloc[-100:].values

            # Get predicted values for the current model and horizon
            y_pred = predictions[dataset][model][horizon]

            # Calculate RMSE
            rmse = calculate_rmse(y_true, y_pred)

            # Store the results
            rmse_results.append({
                'Dataset': dataset,
                'Model': model,
                'Horizon': horizon,
                'RMSE': rmse
            })

            # Plot actual vs predicted
            plot_actual_vs_predicted(
                df_gas if dataset == 'Gas' else df_water if dataset == 'Water' else df_building,
                y_pred,
                model_name=model,
                dataset_name=dataset,
                horizon=horizon
            )

# Convert RMSE results to DataFrame for easy comparison
df_rmse = pd.DataFrame(rmse_results)

# Display RMSE results
print("RMSE Results for All Models, Datasets, and Time Periods:")
print(df_rmse)

# Find the best model based on RMSE (lowest RMSE)
best_model_row = df_rmse.loc[df_rmse['RMSE'].idxmin()]
print(f"\nBest Model: {best_model_row['Model']} for {best_model_row['Dataset']} ({best_model_row['Horizon']}) with RMSE: {best_model_row['RMSE']}")

def plot_residuals(df, y_pred, model_name, dataset_name, horizon):
    """
    Plot residuals (Actual - Predicted).
    """
    y_true = df['consumption']
    residuals = y_true - y_pred
    plt.figure(figsize=(12, 6))
    plt.plot(residuals, label='Residuals', color='red')
    plt.axhline(y=0, color='black', linestyle='--')
    plt.title(f'Residuals for {model_name} - {dataset_name} - {horizon}', fontsize=14)
    plt.xlabel('Time Steps', fontsize=12)
    plt.ylabel('Residuals', fontsize=12)
    plt.legend()
    plt.grid(True)
    plt.show()

def plot_rmse_over_time(df_rmse):
    """
    Plot RMSE over different time periods (1d, 7d, 30d).
    """
    plt.figure(figsize=(12, 6))
    for dataset in df_rmse['Dataset'].unique():
        for model in df_rmse['Model'].unique():
            subset = df_rmse[(df_rmse['Dataset'] == dataset) & (df_rmse['Model'] == model)]
            plt.plot(subset['Horizon'], subset['RMSE'], label=f'{model} - {dataset}')

    plt.title('Model Performance (RMSE) Over Time Periods', fontsize=14)
    plt.xlabel('Horizon (Time Period)', fontsize=12)
    plt.ylabel('RMSE', fontsize=12)
    plt.legend()
    plt.grid(True)
    plt.show()

def plot_cumulative_error(df, y_pred, model_name, dataset_name, horizon):
    """
    Plot cumulative prediction error over time.
    """
    y_true = df['consumption'].iloc[-len(y_pred):].values
    residuals = y_true - y_pred
    cumulative_error = np.cumsum(residuals)

    plt.figure(figsize=(12, 5))
    plt.plot(cumulative_error, label='Cumulative Error', color='purple')
    plt.title(f'Cumulative Error for {model_name} - {dataset_name} - {horizon}', fontsize=14)
    plt.xlabel('Time Steps', fontsize=12)
    plt.ylabel('Cumulative Error', fontsize=12)
    plt.legend()
    plt.grid(True)
    plt.show()